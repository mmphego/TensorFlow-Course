{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlV99VlSFdEM"
      },
      "source": [
        "# TensorFlow Tutorial Using Fashion MNIST\n",
        "![clothes dataset images](https://github.com/mmphego/TensorFlow-Course/raw/master/img/clothes.png)\n",
        "*Figure 1 [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) samples (by Zalando, MIT License).*\n",
        "\n",
        "## Introduction\n",
        "Machine learning (ML)/Neural Network (NN) tools have recently made a huge splash with applications in data analysis, image classification, and data generation. Although ML methods have existed for decades, recent advancements in hardware have generated systems powerful enough to run these algorithms.\n",
        "\n",
        "The typical \"hello world\" example for ML is a classifier trained over the MNIST dataset; a dataset of the handwritten digits 0-9. This dataset is getting a little stale and is no longer impressive with employers as a proof of capability due to both its seeming simplicity and to the plethora of existing tutorials on the topic. Here we will use a newer dataset to perform our ML \"hello world\", the Fashion MNIST dataset!\n",
        "\n",
        "The Zeroth Step of ML (that should be completed before ever putting a hand to mouse, or finger to key) is understanding the format and sizes of your data. This step is often referred to as feature engineering. Feature engineering, typically, includes selecting and preprocessing the particular aspects of training data to give to your algorithm. You and I will start a good habit of examining the data and its format to make decisions concerning the appropriate size and format for our NN.\n",
        "\n",
        "The Fashion MNIST dataset is comprised of 70,000 grayscale images of articles of clothing. The greyscale values for a pixel range from 0-255 (black to white). Each low-resolution image is 28x28 pixels and is of exactly one clothing item. Alongside each image is a label that places the article within a category; these categories are shown in Figure 2 with an example image belonging to the class.\n",
        "\n",
        "![detail view of clothing categories](https://github.com/mmphego/TensorFlow-Course/raw/master/img/clothesDetails.png)\n",
        "*Figure 2 class numbers are shown next to image labels*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubvVFU8u4BGx"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sivO7liBEyri"
      },
      "source": [
        "## Import MNIST\n",
        "\n",
        "Now we are ready to roll! First, we must admit that it takes a lot of data to train a NN, and 70,000 examples is an anemic dataset. So instead of doing a more traditional 70/20/10 or 80/10/10 percent split between training/validating/testing, we will do a simple 6:1 ratio of training:testing (note that this is not best practices, but when there is limited data it may be your only recourse).\n",
        "\n",
        "We first load in the dataset from the Keras package:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpZP4VA04fEf",
        "outputId": "0295045f-c3f1-45ad-90c9-f3bc4c417b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWY5ai1_E422"
      },
      "source": [
        "The first line merely assigns the name fashion_mnist to a particular dataset located within Keras' dataset library. The second line defines four arrays containing the training and testing data, cleaved again into separate structures for images and labels, and then loads all of that data into our standup of Python. The training data arrays will be used to --you guessed it-- train the model, and the testing arrays will allow us to evaluate the performance of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4O-9_nkE6IB"
      },
      "source": [
        "## Look at data\n",
        "\n",
        "It's always nice to be able to show that we've actually done something; ever since kindergarten there has been no better way than with a picture! You'll note that we pip installed and imported MatPlotLib, a library for plots and graphs. Here we'll use it to visualize an example of the Fashion MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Qq4Umh5Usy",
        "outputId": "7f18a03f-8611-48a4-8fa2-5f5abc731296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(f\"Train images dimensions: {train_images.shape}\")\n",
        "print(f\"Test images dimensions: {test_images.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images dimensions: (60000, 28, 28)\n",
            "Test images dimensions: (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP6yhxmrFJUr"
      },
      "source": [
        "The first command basically generates a figure object that will be manipulated by commands 2 through 4. Command 2 specifies what it is that we shall be plotting: the first element from the train_images array. NOTE: Recall that python is an inclusive counting language, meaning that it numbers/indexes things starting from zero, not one! And the final command, \"show()\", tells Python to generate this figure in an external (from CMD) window.\n",
        "\n",
        "Your window should contain a plot that looks similar to Figure 3. Also, be aware that after plt.show(), Python will not return you to a command line until the newly generated window (containing our super nice picture) is closed. Upon closing the window, you will be able to continue entering Python commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbEXfoNMFFPq",
        "outputId": "0bc08d29-5b75-42d6-b227-02fcae45d4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colormaps()\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP6cSC1O-4Ky"
      },
      "source": [
        "## Preprocessing the dataset\n",
        "\n",
        "The greyscale assigned to each pixel within an image has a value range of 0-255. We will want to flatten (smoosh… scale…) this range to 0-1. To achieve this flattening, we will exploit the data structure that our images are stored in, arrays. You see, each image is stored as a 2-dimensional array where each numerical value in the array is the greyscale code of particular pixel. Conveniently, if we divide an entire array by a scalar we generate a new array whose elements are the original elements divided by the scalar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL43CmA95KgT"
      },
      "source": [
        "train_images = train_images /255.\n",
        "test_images = test_images / 255"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58StumPg_Gan"
      },
      "source": [
        "## Model Generation\n",
        "\n",
        "Every NN is constructed from a series of connected layers that are full of connection nodes. Simple mathematical operations are undertaken at each node in each layer, yet through the volume of connections and operations, these ML models can perform impressive and complex tasks.\n",
        "\n",
        "Our model will be constructed from 3 layers. The first layer – often referred to as the Input Layer – will intake an image and format the data structure in a method acceptable for the subsequent layers. In our case, this first layer will be a Flatten layer that intakes a multi-dimensional array and produces an array of a single dimension, this places all the pixel data on an equal depth during input. Both of the next layers will be simple fully connected layers, referred to as Dense layers, with 128 and 10 nodes respectively. These fully connected layers are the simplest layer in the sense of understanding, yet allow for the greatest number of layer-to-layer connections and relationships.\n",
        "\n",
        "The final bit of hyper-technical knowledge you'll need to learn is that each layer can have its own particular mathematical operation. These activation functions determine the form and relationship between the information provided by the layer. The first dense layer will feature a Rectified Linear Unit (ReLU) Activation Function that outputs values between zero and 1; mathematically, the activation function behaves like f(x)=max(0,x). The final layer uses the softmax activation function. This function also produces values in the 0-1 range, BUT generates these values such that the sum of the outputs will be 1! This makes the softmax a layer that is excellent at outputting probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JePMyRW_5EOL"
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)),\n",
        "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUohrmbh_LLv"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Models must be both compiled and trained prior to use. When compiling we must define a few more parameters that control how models are updated during training (optimizer), how the model's accuracy is measured during training (loss function), and what is to be measured to determine the model's accuracy (metrics). These values were selected for this project, yet are generally dependent on the model's intent and expected input and output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_OhEwLf6ReB"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I30yyGwW_O2H"
      },
      "source": [
        "Now we can begin training our model! Now, with already having generated and compiled the model, the code required to train the model is a single line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_57YoEpt65X4",
        "outputId": "7222be45-9452-4df5-8f33-143031057373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5013 - accuracy: 0.8243\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3748 - accuracy: 0.8653\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3360 - accuracy: 0.8773\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3106 - accuracy: 0.8867\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2929 - accuracy: 0.8918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fae0eb49c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sie7ToXr_OGX"
      },
      "source": [
        "This single line completes the entire job of training our model, but let's take a brief look at the arguments provided to the `model.fit` command.\n",
        "\n",
        "- The first argument is input data, and recall that our input Flatten layer takes a (28,28) array, conforming to the dimensionality of our images.\n",
        "- Next we train the system by providing the correct classification for all the training examples.\n",
        "- The final argument is the number of epochs undertaken during training; each epoch is a training cycle over all the training data. Our setting the epoch value to 5 means that the model will be trained overall 60,000 training examples 5 times. After each epoch, we get both the value of the loss function and the model's accuracy (88.97% after epoch 5) at this epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mndcx2SBZC4"
      },
      "source": [
        "## Evaluating Our Model\n",
        "\n",
        "Now we are working with a functional and trained NN model. Following our logic from the top, we have built a NN that intakes a (28,28) array, flattens the data into a (784) array, compiled and trained 2 dense layers, and the softmax activation function of the final output layer will provide a probability that the image belongs to each of the 10 label categories.\n",
        "\n",
        "Our model can be evaluated by using the `model.evaluate` command, that takes in the images and labels so that it can compare its predictions to the ground truth provided by the labels. `model.evaluate` provides two outputs, the value of the loss function over the testing examples, and the accuracy of the model over this testing population. The important output for us is the model's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zPZAVukALaX",
        "outputId": "af58b75f-3325-4b81-9a1f-a0e147dd266b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Model Accuracy: {test_acc * 100}%\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3661 - accuracy: 0.8696\n",
            "Model Accuracy: 86.9599997997284%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMn-Kkx_BYSz"
      },
      "source": [
        "This is great! Our model performs at an accuracy of 87.21%. As good as that is, it is lower than the model accuracy promised above (89.01%). This lower performance is due to the model overfitting on the training data. Overfitting occurs when there are too many parameters within the model when compared to the number of training instances; this allows the model to over learn on those limited examples. Overfitting leads to better model performance over non-training data.\n",
        "\n",
        "That said, 87.21% is a decent number! Let's finally learn how you can feed our model the series of test examples from the `test_images` array, and have it provide its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocKAVzN5AiDP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBiq6YxA_wA0",
        "outputId": "99bffb95-33ec-4532-80e1-a31e6ec86e54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "predictions = model.predict(test_images)\n",
        "predictions[1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.5909148e-02, 6.8095360e-08, 8.7986720e-01, 1.5188994e-07,\n",
              "       3.4150165e-02, 1.3426678e-13, 6.0012016e-02, 7.6524100e-17,\n",
              "       6.1393403e-05, 7.7621926e-13], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPc5-TFcDMtW"
      },
      "source": [
        "As we can see, most of the entries in our prediction array are very close to 0. They are written in scientific notation--the value after the e being the number decimal places to adjust the value (for example 5.1 e-04 is actually 0.00051). The entry that stands out is `predictions[0][9]` at .8658, or 86.58%, certainty that this image should be classified as a boot!\n",
        "\n",
        "If you prefer to not look through a list to determine the class label, we can simplify the output by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qL62gpG6_3c",
        "outputId": "7dc5977c-cbdb-406a-b617-346c41423694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argmax(predictions[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhclaNb3Dj4T"
      },
      "source": [
        "Finally, we can verify this prediction by looking at the label ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLjRFFrgDmUL",
        "outputId": "7e206ccb-2fff-4fa5-eb1d-96a52c3647d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_labels[1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-MCYc6OECPw",
        "outputId": "62b3f5df-609c-455c-f49b-ebfe3d34ccd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(test_images[1])\n",
        "plt.colormaps()\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhwA2itYD91r"
      },
      "source": [
        "There you have it! You have built and trained your first neural network from scratch, and properly classified a shirt as a shirt!\n"
      ]
    }
  ]
}